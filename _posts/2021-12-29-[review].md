---
title: "Structural Deep Network Embedding"
categories:
  - aciform
classes: wide
excerpt: "A page with `classes: wide` set to expand the main content's width."
tags: 
  - review
  - embedding
  - graph
---

When using `layout: single` add the following front matter to a page or post to widen the main content:

```yaml
classes: wide
```

# Structural Deep Network Embedding

## Abstract

how to find a method that is able to effectively capture the highly non-linear network structure and preserve the global and local structure is an open yet important problem.

non-linear한 구조를 파악하고, global과 local한 구조를 보존할 수 있는 문제는 아직 해결되지 않았다.

그에 대한 방법으로 SDNE를 제안한다.

1. semi-supervised model 사용 

   multiple layers of non-linear functions

2. exploit the first-order and second-order proximity jointly to preserve the network structure

   

## 1. Introduction

- networks는 어디나 있다 

- One of the fundamental problems is how to learn useful network representations.

- Great challenges

  - High non-linearity 
  - Structure-preserving 
  - Sparsity 

- 이런 문제를 해결하기 위해 제안된 방법론들 

  - IsoMAP

    > **ISOMAP** 은 manifold 에서의 점들 간의 거리를 nearest neighbor graph 에서의 점들 간의 최단 경로로 정의합니다. 그림 (b) 처럼 표면을 따라 이동하는 거리로 두 점 사이의 거리를 정의합니다. 그리고 이 정보를 보존하는 2 차원 임베딩 공간을 학습합니다.

  - Laplacian Eigenmaps 

    > SDNE에서도 등장하는 해당 임베딩은, Edge weigh가 높을 수록 두 노드간의 거리가 가까워질 수 있는 것에 집중한 방법론입입니다. Normalized laplacian matrix를 기반으로 계산되며 아래 수식을 최소화하는 방향으로 학습하게 됩니다.

  - Kernel

  하지만 모든 모델들은 피상적인 모델(shallow model)들이었고 cannot capture the highly non-linear structure well

  

- In order to capture the highly non-linear structure well, in this paper we propose a new deep model to learn vertex representations for networks

  - 비선형성을 잘 파악하기 위해서  vertex representations for networks를 하는 딥러닝 모델을 제안 

- In order to address the structure-preserving and sparsity problems in the deep model, we further propose to exploit the first-order and second-order proximity 

  - Structure-preserving과 Sparsity 문제를 해결하기 위해서 first-order and second-order proximity를 이용 [26] 논문 참조

  > J. Tang, M. Qu, M. Wang, M. Zhang, J. Yan, and Q. Mei. Line: Large-scale information network embedding. In Proceedings of the 24th International Conference on World Wide Web, pages 1067–1077. International World Wide Web Conferences Steering Committee, 2015.

- semi-supervised learning 활용 



## 2. Related work 

### Deep Neural Network

Deep Neural Network는 강력한 powerful representations abilities

But they only focus on one-order information.

> 어떤 의미일까? 
>
> one-order information

### Network Embedding 

- Our work solves the problem of network embedding, which aims to learn representations for networks.

Local Linear Embedding(LLE)

> eigenvectors 어떻게 이용하는거지? 

최근에  local and global network structure 각자 loss functions 사용 

- Deep Walk = random walk and skip-gram 

>  그래프 형태의 데이터를 효과적으로 임베딩할 수 있는 방법 중 하나인 **`DeepWalk`** 에 대해서 써보려고 합니다. 정확히 말하자면 그래프의 `노드` 를 임베딩할 수 있는 방법인데요. 핵심을 요약하면 다음과 같습니다.
>
>  `skip-gram` 은 `word2vec` 의 방법 중 하나입니다. 대상 단어와 주변 단어를 관계를 이용해 함께 자주 등장한 단어일수록 유사한 백터공간에 표현하는 것이 목적입니다. 입력은 단어 뭉치이고 학습을 완료했을 때 최종 결과 값은 가중치 행렬입니다. 이 가중치 행렬은 각 단어의 벡터 값을 포함하고 있습니다.
>
>  skip gram 은 **대상 단어가 입력으로 들어왔을 때 주변 단어를 예측하는 식**으로 학습이 진행됩니다. `window size` 는 대상 단어가 주어졌을 때 주변 단어를 몇 개 까지 확인할 것인지를 나타내는 하이퍼파라미터 입니다. 만약 window size 가 2라면 대상 단어의 양 옆 2개의 단어까지를 학습합니다.



## 3. Structural Deep Network Embedding

### Problem Definition

Network Embedding aims to map the graphs data into a low dimensional latent space 

- DEFINITION 1. (Graph) A graph is denoted as G = (V, E), where V = {v1, ..., vn} represents n vertexes and E = {ei,j} n i,j=1 represents the edges. Each edge ei,j is associated with a weight si,j ≥ 0 1 . For vi and vj not linked by an edge, si,j = 0. Otherwise, for unweighted graph si,j = 1 and for weighted graph, si,j > 0.

Network embedding aims to map the graph data into a low dimensional latent space, where each vertex is represented as a low-dimensional vector and the network computing can be directly realized.

네트워크 임베딩의 목적은 컴퓨터에서 계산하기 위해 graph 데이터를  low dimensional latent로 바꾸는데 목적이 있다. 

- Definition 2. (First-Order Proximity) The first-order proximity describes the pairwise proximity between vertexes. For any pair of vertexes, if si,j > 0, there exists positive first-order proximity between vi and vj . Otherwise, the first-order proximity between vi and vj is 0



- Definition 3. (Second-Order Proximity) The second-order proximity between a pair of vertexes describes the proximity of the pair’s neighborhood structure. Let Nu = {su,1, ..., su,|V |} denote the first-order proximity between vu and other vertexes. Then, secondorder proximity is determined by the similarity of Nu and Nv.

  

- Definition 4. (Network Embedding) Given a graph denoted as G = (V, E), network embedding aims to learn a mapping function f : vi 7−→ yi ∈ R d , where d  |V |. The objective of the function is to make the similarity between yi and yj explicitly preserve the first-order and second-order proximity of vi and vj .



### The Model

- Framework

![image-20211228152152489](C:\Users\JHLee\AppData\Roaming\Typora\typora-user-images\image-20211228152152489.png)

- Loss Functions

Before introducing the loss functions, we define some of the terms and notations in Table 1 which will be used later. Note thatˆ above the parameters represents the parameters of the decoder.







## 4. Experiments

### Datasets 

### Baseline Algorithms 

### Evaluation Metrics

### Parameter Settings

### Experiment Results

- Network Reconstruction
- Multi-label Classification
- Link Prediction
- Visualization





## 5. Conclusions







## ? 모르는거 

>  exploit the first-order and second-order proximity 

> The first-order proximity



## 관련 자료 

Graph Embeddings — The Summary: 

https://towardsdatascience.com/graph-embeddings-the-summary-cc6075aba007

> 1차 근접성: 모서리로 연결된 노드 간의 로컬 쌍별 유사도 
>
> 2창 근접성: 노드의 이웃 구조의 유사성 
>
> 두 노드가 많은 이웃을 공유하는 경우 유사한 경향이 있다.
>
> **skip-gram 모델 훈련** . 그래프는 문서와 유사합니다. 문서는 단어의 집합이므로 그래프는 하위 그래프의 집합입니다. 이 단계에서 skip-gram 모델이 학습됩니다. 입력에 대한 그래프에 존재하는 하위 그래프의 예측 확률을 최대화하도록 훈련됩니다. 

